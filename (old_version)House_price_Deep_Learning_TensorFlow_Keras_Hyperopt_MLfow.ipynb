{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63fd6b70",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "The Purpose of my repos is to learn, in this repo I try to learn how to use TensorFlow Keras, Hyperopt, MLflow to develop a deep learning model\n",
    "\n",
    "It includes the following steps: \n",
    "- STEP 1: DATA PREP\n",
    "Load and preprocess data \n",
    "- STEP 2: Neural Network Model\n",
    "        Part 1. Create a neural network model with TensorFlow Keras and view training with inline TensorBoard\n",
    "        Part 2. Perform automated hyperparameter tuning with Hyperopt and MLflow and use autologging to save results\n",
    "        Part 3. Use the best set of hyperparameters to build a final model\n",
    "        Part 4. Register the model in MLflow and use the model to make predictions\n",
    "\n",
    "This repo follow instructions notebooks provided on DataBricks websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23314c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.9\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8bb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow libraries, mlflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import mlflow.tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372e757",
   "metadata": {},
   "source": [
    "## DATA PREP\n",
    "Using California Housing dataset scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d36ff",
   "metadata": {},
   "source": [
    "### Load and train-test-split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb63e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_cal_housing = fetch_california_housing()\n",
    "\n",
    "#split train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_cal_housing.data, df_cal_housing.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8431c7a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   7.9887    ,   15.        ,    6.83713355, ...,    2.84364821,\n",
       "          33.69      , -117.8       ],\n",
       "       [   4.3164    ,    6.        ,    3.35346756, ...,    3.92393736,\n",
       "          33.99      , -118.22      ],\n",
       "       [   2.3409    ,   38.        ,    4.71863118, ...,    4.01140684,\n",
       "          34.08      , -118.19      ],\n",
       "       ...,\n",
       "       [   5.0853    ,   26.        ,    5.91649695, ...,    2.95723014,\n",
       "          33.68      , -117.89      ],\n",
       "       [   4.8405    ,   17.        ,    6.03267974, ...,    2.7486174 ,\n",
       "          37.43      , -122.43      ],\n",
       "       [   2.1458    ,   40.        ,    5.5621118 , ...,    2.10869565,\n",
       "          37.64      , -120.98      ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142c710",
   "metadata": {},
   "source": [
    "### Scale features\n",
    "Feature scaling is important when working with neural networks, we will use StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f4cb1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test) #scaler is fitted by X_train already, using transform only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc128f",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45808b",
   "metadata": {},
   "source": [
    "#### Part 1. Create model and view TensorBoard in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e7b33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #relu = rectified linear activation function - looks and acts like a linear function, \n",
    "    #but is, in fact, a nonlinear function allowing complex relationships in the data to be learned.\n",
    "    #Dense is layer, is deeply connected with its preceding layer \n",
    "    # which means the neurons of the layer are connected to every neuron of its preceding layer. \n",
    "    model.add(Dense(20, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10166385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 23:50:36.813812: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-04 23:50:36.814246: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#compile the model\n",
    "model = create_model()\n",
    "#----\n",
    "model.compile(loss='mse',\n",
    "             optimizer='Adam',\n",
    "             metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ba8bd",
   "metadata": {},
   "source": [
    "#### callbacks\n",
    "callbacks are the special utilities or functions that are executed during training at given stages of the training procedure. Callbacks can help you prevent overfitting, visualize training progress, debug your code, save checkpoints, generate logs, create a TensorBoard, etc. There are many callbacks readily available in TensorFlow, and you can use multiple. \n",
    "https://blog.paperspace.com/tensorflow-callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8451cc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 00:33:13.241558: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-03 00:33:13.440479: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - ETA: 0s - loss: 1.2445 - mse: 1.2445"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 00:33:16.566886: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.56320, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 1.2445 - mse: 1.2445 - val_loss: 0.5632 - val_mse: 0.5632\n",
      "Epoch 2/50\n",
      "407/413 [============================>.] - ETA: 0s - loss: 0.5213 - mse: 0.5213\n",
      "Epoch 2: val_loss improved from 0.56320 to 0.42794, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.5187 - mse: 0.5187 - val_loss: 0.4279 - val_mse: 0.4279\n",
      "Epoch 3/50\n",
      "406/413 [============================>.] - ETA: 0s - loss: 0.4325 - mse: 0.4325\n",
      "Epoch 3: val_loss improved from 0.42794 to 0.37594, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.4313 - mse: 0.4313 - val_loss: 0.3759 - val_mse: 0.3759\n",
      "Epoch 4/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.4021 - mse: 0.4021\n",
      "Epoch 4: val_loss improved from 0.37594 to 0.35646, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 0.4013 - mse: 0.4013 - val_loss: 0.3565 - val_mse: 0.3565\n",
      "Epoch 5/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.3836 - mse: 0.3836\n",
      "Epoch 5: val_loss improved from 0.35646 to 0.34409, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3833 - mse: 0.3833 - val_loss: 0.3441 - val_mse: 0.3441\n",
      "Epoch 6/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.3754 - mse: 0.3754\n",
      "Epoch 6: val_loss improved from 0.34409 to 0.33865, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3744 - mse: 0.3744 - val_loss: 0.3386 - val_mse: 0.3386\n",
      "Epoch 7/50\n",
      "406/413 [============================>.] - ETA: 0s - loss: 0.3685 - mse: 0.3685\n",
      "Epoch 7: val_loss improved from 0.33865 to 0.33078, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3670 - mse: 0.3670 - val_loss: 0.3308 - val_mse: 0.3308\n",
      "Epoch 8/50\n",
      "407/413 [============================>.] - ETA: 0s - loss: 0.3635 - mse: 0.3635\n",
      "Epoch 8: val_loss improved from 0.33078 to 0.32756, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3631 - mse: 0.3631 - val_loss: 0.3276 - val_mse: 0.3276\n",
      "Epoch 9/50\n",
      "406/413 [============================>.] - ETA: 0s - loss: 0.3544 - mse: 0.3544\n",
      "Epoch 9: val_loss improved from 0.32756 to 0.32398, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 0.3535 - mse: 0.3535 - val_loss: 0.3240 - val_mse: 0.3240\n",
      "Epoch 10/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.3475 - mse: 0.3475\n",
      "Epoch 10: val_loss did not improve from 0.32398\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.3473 - mse: 0.3473 - val_loss: 0.3258 - val_mse: 0.3258\n",
      "Epoch 11/50\n",
      "413/413 [==============================] - ETA: 0s - loss: 0.3426 - mse: 0.3426\n",
      "Epoch 11: val_loss did not improve from 0.32398\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3426 - mse: 0.3426 - val_loss: 0.3277 - val_mse: 0.3277\n",
      "Epoch 12/50\n",
      "412/413 [============================>.] - ETA: 0s - loss: 0.3400 - mse: 0.3400\n",
      "Epoch 12: val_loss improved from 0.32398 to 0.31707, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 0.3398 - mse: 0.3398 - val_loss: 0.3171 - val_mse: 0.3171\n",
      "Epoch 13/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.3351 - mse: 0.3351\n",
      "Epoch 13: val_loss improved from 0.31707 to 0.31641, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3350 - mse: 0.3350 - val_loss: 0.3164 - val_mse: 0.3164\n",
      "Epoch 14/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.3340 - mse: 0.3340\n",
      "Epoch 14: val_loss did not improve from 0.31641\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3336 - mse: 0.3336 - val_loss: 0.3168 - val_mse: 0.3168\n",
      "Epoch 15/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.3306 - mse: 0.3306\n",
      "Epoch 15: val_loss improved from 0.31641 to 0.30862, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3309 - mse: 0.3309 - val_loss: 0.3086 - val_mse: 0.3086\n",
      "Epoch 16/50\n",
      "412/413 [============================>.] - ETA: 0s - loss: 0.3266 - mse: 0.3266\n",
      "Epoch 16: val_loss improved from 0.30862 to 0.30768, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3264 - mse: 0.3264 - val_loss: 0.3077 - val_mse: 0.3077\n",
      "Epoch 17/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.3333 - mse: 0.3333\n",
      "Epoch 17: val_loss did not improve from 0.30768\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3328 - mse: 0.3328 - val_loss: 0.3122 - val_mse: 0.3122\n",
      "Epoch 18/50\n",
      "412/413 [============================>.] - ETA: 0s - loss: 0.3211 - mse: 0.3211\n",
      "Epoch 18: val_loss improved from 0.30768 to 0.30503, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 4s 8ms/step - loss: 0.3211 - mse: 0.3211 - val_loss: 0.3050 - val_mse: 0.3050\n",
      "Epoch 19/50\n",
      "410/413 [============================>.] - ETA: 0s - loss: 0.3174 - mse: 0.3174\n",
      "Epoch 19: val_loss improved from 0.30503 to 0.30114, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 0.3171 - mse: 0.3171 - val_loss: 0.3011 - val_mse: 0.3011\n",
      "Epoch 20/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.3120 - mse: 0.3120\n",
      "Epoch 20: val_loss improved from 0.30114 to 0.30030, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3129 - mse: 0.3129 - val_loss: 0.3003 - val_mse: 0.3003\n",
      "Epoch 21/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.3119 - mse: 0.3119\n",
      "Epoch 21: val_loss improved from 0.30030 to 0.29947, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3121 - mse: 0.3121 - val_loss: 0.2995 - val_mse: 0.2995\n",
      "Epoch 22/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.3120 - mse: 0.3120\n",
      "Epoch 22: val_loss did not improve from 0.29947\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3117 - mse: 0.3117 - val_loss: 0.3020 - val_mse: 0.3020\n",
      "Epoch 23/50\n",
      "407/413 [============================>.] - ETA: 0s - loss: 0.3097 - mse: 0.3097\n",
      "Epoch 23: val_loss improved from 0.29947 to 0.29557, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3087 - mse: 0.3087 - val_loss: 0.2956 - val_mse: 0.2956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.3068 - mse: 0.3068\n",
      "Epoch 24: val_loss improved from 0.29557 to 0.29487, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3060 - mse: 0.3060 - val_loss: 0.2949 - val_mse: 0.2949\n",
      "Epoch 25/50\n",
      "413/413 [==============================] - ETA: 0s - loss: 0.3059 - mse: 0.3059\n",
      "Epoch 25: val_loss improved from 0.29487 to 0.29340, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3059 - mse: 0.3059 - val_loss: 0.2934 - val_mse: 0.2934\n",
      "Epoch 26/50\n",
      "411/413 [============================>.] - ETA: 0s - loss: 0.3030 - mse: 0.3030\n",
      "Epoch 26: val_loss did not improve from 0.29340\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.3030 - mse: 0.3030 - val_loss: 0.2955 - val_mse: 0.2955\n",
      "Epoch 27/50\n",
      "407/413 [============================>.] - ETA: 0s - loss: 0.3099 - mse: 0.3099\n",
      "Epoch 27: val_loss improved from 0.29340 to 0.28998, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 0.3093 - mse: 0.3093 - val_loss: 0.2900 - val_mse: 0.2900\n",
      "Epoch 28/50\n",
      "412/413 [============================>.] - ETA: 0s - loss: 0.2988 - mse: 0.2988\n",
      "Epoch 28: val_loss did not improve from 0.28998\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2988 - mse: 0.2988 - val_loss: 0.2915 - val_mse: 0.2915\n",
      "Epoch 29/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2948 - mse: 0.2948\n",
      "Epoch 29: val_loss improved from 0.28998 to 0.28648, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2964 - mse: 0.2964 - val_loss: 0.2865 - val_mse: 0.2865\n",
      "Epoch 30/50\n",
      "407/413 [============================>.] - ETA: 0s - loss: 0.2951 - mse: 0.2951\n",
      "Epoch 30: val_loss improved from 0.28648 to 0.28631, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 0.2938 - mse: 0.2938 - val_loss: 0.2863 - val_mse: 0.2863\n",
      "Epoch 31/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2943 - mse: 0.2943\n",
      "Epoch 31: val_loss did not improve from 0.28631\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2941 - mse: 0.2941 - val_loss: 0.2888 - val_mse: 0.2888\n",
      "Epoch 32/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.2919 - mse: 0.2919\n",
      "Epoch 32: val_loss improved from 0.28631 to 0.28154, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2932 - mse: 0.2932 - val_loss: 0.2815 - val_mse: 0.2815\n",
      "Epoch 33/50\n",
      "413/413 [==============================] - ETA: 0s - loss: 0.2975 - mse: 0.2975\n",
      "Epoch 33: val_loss did not improve from 0.28154\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2975 - mse: 0.2975 - val_loss: 0.2863 - val_mse: 0.2863\n",
      "Epoch 34/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.2888 - mse: 0.2888\n",
      "Epoch 34: val_loss did not improve from 0.28154\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2898 - mse: 0.2898 - val_loss: 0.2902 - val_mse: 0.2902\n",
      "Epoch 35/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.2880 - mse: 0.2880\n",
      "Epoch 35: val_loss improved from 0.28154 to 0.28124, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2888 - mse: 0.2888 - val_loss: 0.2812 - val_mse: 0.2812\n",
      "Epoch 36/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2888 - mse: 0.2888\n",
      "Epoch 36: val_loss did not improve from 0.28124\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2884 - mse: 0.2884 - val_loss: 0.3057 - val_mse: 0.3057\n",
      "Epoch 37/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2882 - mse: 0.2882\n",
      "Epoch 37: val_loss did not improve from 0.28124\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2891 - mse: 0.2891 - val_loss: 0.2872 - val_mse: 0.2872\n",
      "Epoch 38/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2842 - mse: 0.2842\n",
      "Epoch 38: val_loss did not improve from 0.28124\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2847 - mse: 0.2847 - val_loss: 0.2848 - val_mse: 0.2848\n",
      "Epoch 39/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2854 - mse: 0.2854\n",
      "Epoch 39: val_loss improved from 0.28124 to 0.27922, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2850 - mse: 0.2850 - val_loss: 0.2792 - val_mse: 0.2792\n",
      "Epoch 40/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2863 - mse: 0.2863\n",
      "Epoch 40: val_loss did not improve from 0.27922\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2859 - mse: 0.2859 - val_loss: 0.2796 - val_mse: 0.2796\n",
      "Epoch 41/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2820 - mse: 0.2820\n",
      "Epoch 41: val_loss improved from 0.27922 to 0.27811, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2839 - mse: 0.2839 - val_loss: 0.2781 - val_mse: 0.2781\n",
      "Epoch 42/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2864 - mse: 0.2864\n",
      "Epoch 42: val_loss did not improve from 0.27811\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2863 - mse: 0.2863 - val_loss: 0.2812 - val_mse: 0.2812\n",
      "Epoch 43/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2808 - mse: 0.2808\n",
      "Epoch 43: val_loss did not improve from 0.27811\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2810 - mse: 0.2810 - val_loss: 0.2825 - val_mse: 0.2825\n",
      "Epoch 44/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2809 - mse: 0.2809\n",
      "Epoch 44: val_loss did not improve from 0.27811\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2813 - mse: 0.2813 - val_loss: 0.2856 - val_mse: 0.2856\n",
      "Epoch 45/50\n",
      "407/413 [============================>.] - ETA: 0s - loss: 0.2831 - mse: 0.2831\n",
      "Epoch 45: val_loss did not improve from 0.27811\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2826 - mse: 0.2826 - val_loss: 0.2812 - val_mse: 0.2812\n",
      "Epoch 46/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.2796 - mse: 0.2796\n",
      "Epoch 46: val_loss did not improve from 0.27811\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2787 - mse: 0.2787 - val_loss: 0.2809 - val_mse: 0.2809\n",
      "Epoch 47/50\n",
      "406/413 [============================>.] - ETA: 0s - loss: 0.2781 - mse: 0.2781\n",
      "Epoch 47: val_loss improved from 0.27811 to 0.27774, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2785 - mse: 0.2785 - val_loss: 0.2777 - val_mse: 0.2777\n",
      "Epoch 48/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2801 - mse: 0.2801\n",
      "Epoch 48: val_loss did not improve from 0.27774\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2805 - mse: 0.2805 - val_loss: 0.2787 - val_mse: 0.2787\n",
      "Epoch 49/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2798 - mse: 0.2798\n",
      "Epoch 49: val_loss improved from 0.27774 to 0.27261, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2793 - mse: 0.2793 - val_loss: 0.2726 - val_mse: 0.2726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2785 - mse: 0.2785\n",
      "Epoch 50: val_loss did not improve from 0.27261\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2777 - mse: 0.2777 - val_loss: 0.2747 - val_mse: 0.2747\n"
     ]
    }
   ],
   "source": [
    "#create callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#directory\n",
    "experiment_log_dir = './DB/tb'\n",
    "checkpoint_path = './DB/keras_checkpoint_weights.ckpt'\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=experiment_log_dir)\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='loss', mode='min', patience=3)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=.2, epochs=35, callbacks=[tensorboard_callback,\n",
    "                                                                                 model_checkpoint,\n",
    "                                                                                early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b594657",
   "metadata": {},
   "source": [
    "#### TensorBoard commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49599856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b6d8b38b961e70d3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b6d8b38b961e70d3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $experiment_log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383a015",
   "metadata": {},
   "source": [
    "#### Evaluate model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17ff48f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 1s 6ms/step - loss: 0.2741 - mse: 0.2741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2741457223892212, 0.2741457223892212]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a546755",
   "metadata": {},
   "source": [
    "### Part 2. Perform automated hyperparameter tuning with Hyperopt\n",
    "Hyperopt's job is to find the best value of a scalar-valued, possibly-stochastic function over a set of possible arguments to that function. Whereas many optimization packages will assume that these inputs are drawn from a vector space, Hyperopt is different in that it encourages you to describe your search space in more detail. By providing more information about where your function is defined, and where you think the best values are, you allow algorithms in hyperopt to search more efficiently.\n",
    "https://github.com/hyperopt/hyperopt/wiki/FMin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff9a90",
   "metadata": {},
   "source": [
    "#### Create neural network model using variables for number of nodes in hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6f9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(n[\"dense_l1\"]), input_dim=8, activation=\"relu\"))\n",
    "    model.add(Dense(int(n[\"dense_l2\"]), activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49bd5ac",
   "metadata": {},
   "source": [
    "#### Create Hyperopt objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bca0662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, hp, tpe, STATUS_OK, SparkTrials\n",
    " \n",
    "def runNN(n):\n",
    "  # Import tensorflow \n",
    "    import tensorflow as tf\n",
    "  #create callbacks\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "    \n",
    "  \n",
    "  # Log run information with mlflow.tensorflow.autolog()\n",
    "    mlflow.tensorflow.autolog()\n",
    "  \n",
    "    model = create_model(n)\n",
    " \n",
    "  # Select optimizer\n",
    "    optimizer_call = getattr(tf.keras.optimizers, n[\"optimizer\"])\n",
    "    optimizer = optimizer_call(learning_rate=n[\"learning_rate\"])\n",
    " \n",
    "  # Compile model\n",
    "    model.compile(loss=\"mse\",\n",
    "                optimizer=optimizer,\n",
    "                metrics=[\"mse\"])\n",
    " \n",
    "    history = model.fit(X_train, y_train, validation_split=.2, \n",
    "                        epochs=35, verbose=2)\n",
    " \n",
    "  # Evaluate the model\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    obj_metric = score[0]  \n",
    "    return {\"loss\": obj_metric, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f3873",
   "metadata": {},
   "source": [
    "#### Define Hyperopt search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a507e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "  \"dense_l1\": hp.quniform(\"dense_l1\", 10, 30, 1),\n",
    "  \"dense_l2\": hp.quniform(\"dense_l2\", 10, 30, 1),\n",
    "  \"learning_rate\": hp.loguniform(\"learning_rate\", -5, 0),\n",
    "  \"optimizer\": hp.choice(\"optimizer\", [\"Adadelta\", \"Adam\"])\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f41188",
   "metadata": {},
   "source": [
    "#### Create the SparkTrials object\n",
    "The SparkTrials object tells fmin() to distrubte the tuning job across a Spark cluster. When we create the SparkTrials object, we can use the parallelism argument to set the maximum number of trials to evaluate concurently. The default setting is the number of Spark executors available\n",
    "\n",
    "A higher number lets you scale-out testing of more hyperparameter settings. Because Hyperopt proposes new trials based on past results. Because Hyperopt proposes new trials based on past results, there is a trade-off between parallelism and adaptivity. For a fixed max_evals, greater parallelism speeds up calculations, but lower parallelism may lead to better results since each iteration has access to more past results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "035c34c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 23:53:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "#Create a spark Context class, with custom config\n",
    "conf = SparkConf()\n",
    "#conf.set('spark.sql.debug.maxToStringFields', 100)\n",
    "conf.set('spark.python.worker.reuse', 'true')\n",
    "conf.set('spark.python.worker.memory', '3g')\n",
    "conf.set('spark.executor.memory', '35g')\n",
    "conf.set('spark.dynamicAllocation.enabled', 'true')\n",
    "conf.set('spark.dynamicAllocation.maxExecutors', 25)\n",
    "conf.set('spark.executor.memoryOverhead', '20')\n",
    "conf.set('spark.driver.memoryOverhead', '30g')\n",
    "conf.set('spark.driver.memoryOverheadFactor', 0.7)\n",
    "conf.set('spark.executor.cores', 8)\n",
    "conf.set('spark.default.parallelism', 700)\n",
    "conf.set('spark.sql.shuffle.partitions', 700)\n",
    "conf.set('spark.driver.memory', '30g')\n",
    "conf.set('spark.driver.cores', 8)\n",
    "#conf.set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.0.0\")\n",
    "#conf.set('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "#conf.set('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5fad0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6da1682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create spark session\n",
    "spark = SparkSession.builder.master('local[*]').\\\n",
    "                config('spark.sql.debug.maxToStringFields', '100').\\\n",
    "                appName(\"Python Spark Dataframes Financial Fruad\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7926855f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hops-air.ht.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac3abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e1dcda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default argument is set\n",
    "spark_trials = SparkTrials(parallelism=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce58f0",
   "metadata": {},
   "source": [
    "#### Perform hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03691444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                  | 0/30 [00:00<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "#Put the fmin() call inside an MLflow run to save results to MLflow. \n",
    "#MLflow trachs the parameters and performance of each run\n",
    "\n",
    "with mlflow.start_run():\n",
    "    best_hyperparam = fmin(fn=runNN, \n",
    "                         space=space, \n",
    "                         algo=tpe.suggest, \n",
    "                         max_evals=30, \n",
    "                         trials=spark_trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "lighthouse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
