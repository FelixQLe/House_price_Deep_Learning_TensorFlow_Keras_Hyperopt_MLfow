{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2d42a0",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "The Purpose of my repos is to learn, in this repo I try to learn how to use TensorFlow Keras, Hyperopt, MLflow to develop a deep learning model\n",
    "\n",
    "It includes the following steps: \n",
    "- STEP 1: DATA PREP\n",
    "Load and preprocess data \n",
    "- STEP 2: Neural Network Model\n",
    "        Part 1. Create a neural network model with TensorFlow Keras and view training with inline TensorBoard\n",
    "        Part 2. Perform automated hyperparameter tuning with Hyperopt and MLflow and use autologging to save results\n",
    "        Part 3. Use the best set of hyperparameters to build a final model\n",
    "        Part 4. Register the model in MLflow and use the model to make predictions\n",
    "\n",
    "This repo follow instructions notebooks provided on DataBricks websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0601162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow libraries, mlflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import mlflow.tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a7b75f",
   "metadata": {},
   "source": [
    "## DATA PREP\n",
    "Using California Housing dataset scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e05f89",
   "metadata": {},
   "source": [
    "### Load and train-test-split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9df15dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_cal_housing = fetch_california_housing()\n",
    "\n",
    "#split train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_cal_housing.data, df_cal_housing.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b669bc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2.1447    ,   26.        ,    3.48595041, ...,    3.50413223,\n",
       "          33.79      , -118.27      ],\n",
       "       [   4.1739    ,   36.        ,    5.45962733, ...,    2.86749482,\n",
       "          34.09      , -117.71      ],\n",
       "       [   3.1641    ,   17.        ,    5.58348624, ...,    2.6733945 ,\n",
       "          35.35      , -120.49      ],\n",
       "       ...,\n",
       "       [   2.63      ,   52.        ,    4.51311953, ...,    3.29737609,\n",
       "          34.07      , -117.75      ],\n",
       "       [   3.3326    ,   52.        ,    3.89162562, ...,    3.60098522,\n",
       "          34.12      , -118.2       ],\n",
       "       [   8.0784    ,   52.        ,    6.88219178, ...,    2.67945205,\n",
       "          33.77      , -117.87      ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6be768",
   "metadata": {},
   "source": [
    "### Scale features\n",
    "Feature scaling is important when working with neural networks, we will use StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1580fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test) #scaler is fitted by X_train already, using transform only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b0615",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20349160",
   "metadata": {},
   "source": [
    "#### Part 1. Create model and view TensorBoard in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a489641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #relu = rectified linear activation function - looks and acts like a linear function, \n",
    "    #but is, in fact, a nonlinear function allowing complex relationships in the data to be learned.\n",
    "    #Dense is layer, is deeply connected with its preceding layer \n",
    "    # which means the neurons of the layer are connected to every neuron of its preceding layer. \n",
    "    model.add(Dense(20, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef3556f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model = create_model()\n",
    "#----\n",
    "model.compile(loss='mse',\n",
    "             optimizer='Adam',\n",
    "             metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee7e3a",
   "metadata": {},
   "source": [
    "#### callbacks\n",
    "callbacks are the special utilities or functions that are executed during training at given stages of the training procedure. Callbacks can help you prevent overfitting, visualize training progress, debug your code, save checkpoints, generate logs, create a TensorBoard, etc. There are many callbacks readily available in TensorFlow, and you can use multiple. \n",
    "https://blog.paperspace.com/tensorflow-callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4356b3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2958 - mse: 0.2958\n",
      "Epoch 1: val_loss improved from inf to 0.29848, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2964 - mse: 0.2964 - val_loss: 0.2985 - val_mse: 0.2985\n",
      "Epoch 2/50\n",
      "412/413 [============================>.] - ETA: 0s - loss: 0.2966 - mse: 0.2966\n",
      "Epoch 2: val_loss improved from 0.29848 to 0.29469, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2964 - mse: 0.2964 - val_loss: 0.2947 - val_mse: 0.2947\n",
      "Epoch 3/50\n",
      "413/413 [==============================] - ETA: 0s - loss: 0.2973 - mse: 0.2973\n",
      "Epoch 3: val_loss improved from 0.29469 to 0.29328, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2973 - mse: 0.2973 - val_loss: 0.2933 - val_mse: 0.2933\n",
      "Epoch 4/50\n",
      "407/413 [============================>.] - ETA: 0s - loss: 0.2944 - mse: 0.2944\n",
      "Epoch 4: val_loss did not improve from 0.29328\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2937 - mse: 0.2937 - val_loss: 0.2976 - val_mse: 0.2976\n",
      "Epoch 5/50\n",
      "406/413 [============================>.] - ETA: 0s - loss: 0.2957 - mse: 0.2957\n",
      "Epoch 5: val_loss improved from 0.29328 to 0.29262, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2952 - mse: 0.2952 - val_loss: 0.2926 - val_mse: 0.2926\n",
      "Epoch 6/50\n",
      "407/413 [============================>.] - ETA: 0s - loss: 0.2958 - mse: 0.2958\n",
      "Epoch 6: val_loss did not improve from 0.29262\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2953 - mse: 0.2953 - val_loss: 0.2929 - val_mse: 0.2929\n",
      "Epoch 7/50\n",
      "407/413 [============================>.] - ETA: 0s - loss: 0.2914 - mse: 0.2914\n",
      "Epoch 7: val_loss did not improve from 0.29262\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2916 - mse: 0.2916 - val_loss: 0.3018 - val_mse: 0.3018\n",
      "Epoch 8/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.2914 - mse: 0.2914\n",
      "Epoch 8: val_loss improved from 0.29262 to 0.29187, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2906 - mse: 0.2906 - val_loss: 0.2919 - val_mse: 0.2919\n",
      "Epoch 9/50\n",
      "411/413 [============================>.] - ETA: 0s - loss: 0.2906 - mse: 0.2906\n",
      "Epoch 9: val_loss improved from 0.29187 to 0.29084, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2908 - mse: 0.2908 - val_loss: 0.2908 - val_mse: 0.2908\n",
      "Epoch 10/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2931 - mse: 0.2931\n",
      "Epoch 10: val_loss did not improve from 0.29084\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2944 - mse: 0.2944 - val_loss: 0.2919 - val_mse: 0.2919\n",
      "Epoch 11/50\n",
      "411/413 [============================>.] - ETA: 0s - loss: 0.2894 - mse: 0.2894\n",
      "Epoch 11: val_loss improved from 0.29084 to 0.28825, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2888 - mse: 0.2888 - val_loss: 0.2882 - val_mse: 0.2882\n",
      "Epoch 12/50\n",
      "413/413 [==============================] - ETA: 0s - loss: 0.2888 - mse: 0.2888\n",
      "Epoch 12: val_loss did not improve from 0.28825\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2888 - mse: 0.2888 - val_loss: 0.2891 - val_mse: 0.2891\n",
      "Epoch 13/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.2896 - mse: 0.2896\n",
      "Epoch 13: val_loss did not improve from 0.28825\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2889 - mse: 0.2889 - val_loss: 0.2883 - val_mse: 0.2883\n",
      "Epoch 14/50\n",
      "411/413 [============================>.] - ETA: 0s - loss: 0.2860 - mse: 0.2860\n",
      "Epoch 14: val_loss did not improve from 0.28825\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2857 - mse: 0.2857 - val_loss: 0.2984 - val_mse: 0.2984\n",
      "Epoch 15/50\n",
      "406/413 [============================>.] - ETA: 0s - loss: 0.2852 - mse: 0.2852\n",
      "Epoch 15: val_loss did not improve from 0.28825\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2855 - mse: 0.2855 - val_loss: 0.3022 - val_mse: 0.3022\n",
      "Epoch 16/50\n",
      "406/413 [============================>.] - ETA: 0s - loss: 0.2865 - mse: 0.2865\n",
      "Epoch 16: val_loss did not improve from 0.28825\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2862 - mse: 0.2862 - val_loss: 0.2905 - val_mse: 0.2905\n",
      "Epoch 17/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.2871 - mse: 0.2871\n",
      "Epoch 17: val_loss did not improve from 0.28825\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2882 - mse: 0.2882 - val_loss: 0.2920 - val_mse: 0.2920\n",
      "Epoch 18/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.2833 - mse: 0.2833\n",
      "Epoch 18: val_loss did not improve from 0.28825\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2834 - mse: 0.2834 - val_loss: 0.2886 - val_mse: 0.2886\n",
      "Epoch 19/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2826 - mse: 0.2826\n",
      "Epoch 19: val_loss did not improve from 0.28825\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2827 - mse: 0.2827 - val_loss: 0.2909 - val_mse: 0.2909\n",
      "Epoch 20/50\n",
      "406/413 [============================>.] - ETA: 0s - loss: 0.2820 - mse: 0.2820\n",
      "Epoch 20: val_loss improved from 0.28825 to 0.28614, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 0.2822 - mse: 0.2822 - val_loss: 0.2861 - val_mse: 0.2861\n",
      "Epoch 21/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.2836 - mse: 0.2836\n",
      "Epoch 21: val_loss did not improve from 0.28614\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2841 - mse: 0.2841 - val_loss: 0.2958 - val_mse: 0.2958\n",
      "Epoch 22/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2934 - mse: 0.2934\n",
      "Epoch 22: val_loss improved from 0.28614 to 0.28439, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2929 - mse: 0.2929 - val_loss: 0.2844 - val_mse: 0.2844\n",
      "Epoch 23/50\n",
      "413/413 [==============================] - ETA: 0s - loss: 0.2804 - mse: 0.2804\n",
      "Epoch 23: val_loss did not improve from 0.28439\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2804 - mse: 0.2804 - val_loss: 0.2898 - val_mse: 0.2898\n",
      "Epoch 24/50\n",
      "412/413 [============================>.] - ETA: 0s - loss: 0.2797 - mse: 0.2797\n",
      "Epoch 24: val_loss improved from 0.28439 to 0.28212, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 0.2797 - mse: 0.2797 - val_loss: 0.2821 - val_mse: 0.2821\n",
      "Epoch 25/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.2784 - mse: 0.2784\n",
      "Epoch 25: val_loss did not improve from 0.28212\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2802 - mse: 0.2802 - val_loss: 0.2918 - val_mse: 0.2918\n",
      "Epoch 26/50\n",
      "413/413 [==============================] - ETA: 0s - loss: 0.2795 - mse: 0.2795\n",
      "Epoch 26: val_loss did not improve from 0.28212\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2795 - mse: 0.2795 - val_loss: 0.2881 - val_mse: 0.2881\n",
      "Epoch 27/50\n",
      "407/413 [============================>.] - ETA: 0s - loss: 0.2777 - mse: 0.2777\n",
      "Epoch 27: val_loss did not improve from 0.28212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2777 - mse: 0.2777 - val_loss: 0.2906 - val_mse: 0.2906\n",
      "Epoch 28/50\n",
      "407/413 [============================>.] - ETA: 0s - loss: 0.2796 - mse: 0.2796\n",
      "Epoch 28: val_loss did not improve from 0.28212\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2786 - mse: 0.2786 - val_loss: 0.2973 - val_mse: 0.2973\n",
      "Epoch 29/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2773 - mse: 0.2773\n",
      "Epoch 29: val_loss did not improve from 0.28212\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2777 - mse: 0.2777 - val_loss: 0.2873 - val_mse: 0.2873\n",
      "Epoch 30/50\n",
      "409/413 [============================>.] - ETA: 0s - loss: 0.2775 - mse: 0.2775\n",
      "Epoch 30: val_loss did not improve from 0.28212\n",
      "413/413 [==============================] - 3s 7ms/step - loss: 0.2790 - mse: 0.2790 - val_loss: 0.2857 - val_mse: 0.2857\n",
      "Epoch 31/50\n",
      "408/413 [============================>.] - ETA: 0s - loss: 0.2770 - mse: 0.2770\n",
      "Epoch 31: val_loss did not improve from 0.28212\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2767 - mse: 0.2767 - val_loss: 0.2837 - val_mse: 0.2837\n",
      "Epoch 32/50\n",
      "407/413 [============================>.] - ETA: 0s - loss: 0.2769 - mse: 0.2769\n",
      "Epoch 32: val_loss did not improve from 0.28212\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2762 - mse: 0.2762 - val_loss: 0.2840 - val_mse: 0.2840\n",
      "Epoch 33/50\n",
      "413/413 [==============================] - ETA: 0s - loss: 0.2774 - mse: 0.2774\n",
      "Epoch 33: val_loss did not improve from 0.28212\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2774 - mse: 0.2774 - val_loss: 0.2883 - val_mse: 0.2883\n",
      "Epoch 34/50\n",
      "413/413 [==============================] - ETA: 0s - loss: 0.2774 - mse: 0.2774\n",
      "Epoch 34: val_loss improved from 0.28212 to 0.28048, saving model to ./DB/keras_checkpoint_weights.ckpt\n",
      "INFO:tensorflow:Assets written to: ./DB/keras_checkpoint_weights.ckpt/assets\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 0.2774 - mse: 0.2774 - val_loss: 0.2805 - val_mse: 0.2805\n",
      "Epoch 35/50\n",
      "412/413 [============================>.] - ETA: 0s - loss: 0.2765 - mse: 0.2765\n",
      "Epoch 35: val_loss did not improve from 0.28048\n",
      "413/413 [==============================] - 3s 8ms/step - loss: 0.2762 - mse: 0.2762 - val_loss: 0.2919 - val_mse: 0.2919\n"
     ]
    }
   ],
   "source": [
    "#create callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "#directory\n",
    "experiment_log_dir = './DB/tb'\n",
    "checkpoint_path = './DB/keras_checkpoint_weights.ckpt'\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=experiment_log_dir)\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='loss', mode='min', patience=3)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=.2, epochs=50, callbacks=[tensorboard_callback,\n",
    "                                                                                 model_checkpoint,\n",
    "                                                                                early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56afc04f",
   "metadata": {},
   "source": [
    "#### TensorBoard commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4360e4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a5b80fe818dd78f9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a5b80fe818dd78f9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $experiment_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728bee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "lighthouse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
